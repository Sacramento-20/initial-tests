{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb48a3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1726 432\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Carrega o corpus de treinamento\n",
    "data = open('shakespeare.txt').read()\n",
    "\n",
    "# Cria uma lista de frases\n",
    "sentences = []\n",
    "\n",
    "# Divide o corpus em frases usando a quebra de linha como delimitador\n",
    "for line in data.split('\\n'):\n",
    "    # Remove os espaços em branco e converte para minúsculo\n",
    "    line = line.strip().lower()\n",
    "    # Adiciona a frase à lista de frases se ela não estiver vazia\n",
    "    if len(line) > 0:\n",
    "        sentences.append(line)\n",
    "\n",
    "# Separa as frases em conjuntos de treinamento e teste\n",
    "train_size = int(len(sentences) * 0.8) # Define a proporção de frases para treinamento\n",
    "training_sentences = sentences[:train_size]\n",
    "testing_sentences = sentences[train_size:]\n",
    "        \n",
    "print(len(training_sentences), len(testing_sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fec4bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cria um objeto Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Ajusta o tokenizer com as frases de treinamento\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "\n",
    "num_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Converte as frases em sequências numéricas\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "\n",
    "# Define o tamanho máximo de sequência\n",
    "max_sequence_len = max([len(x) for x in training_sequences])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84ab4e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preenche as sequências com zeros para ter o mesmo tamanho\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_sequence_len, padding='post')\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_sequence_len, padding='post')\n",
    "\n",
    "training_labels = np.zeros((len(training_sequences), num_words))\n",
    "for i, sequence in enumerate(training_sequences):\n",
    "    for j, word in enumerate(sequence[1:]):\n",
    "        if word < num_words:\n",
    "            training_labels[i, word] = 1\n",
    "\n",
    "testing_labels = np.zeros((len(testing_sequences), num_words))\n",
    "for i, sequence in enumerate(testing_sequences):\n",
    "    for j, word in enumerate(sequence[1:]):\n",
    "        if word < num_words:\n",
    "            testing_labels[i, word] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1ab7d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "# Cria uma rede neural sequencial\n",
    "model = Sequential()\n",
    "\n",
    "# Adiciona uma camada de embedding para representar as palavras\n",
    "model.add(Embedding(input_dim=num_words, output_dim=64))\n",
    "\n",
    "# Adiciona uma camada LSTM para processar as sequências\n",
    "model.add(LSTM(64))\n",
    "\n",
    "# Adiciona uma camada densa para fazer a predição\n",
    "model.add(Dense(num_words, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83231f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-20 14:49:29.404316: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 19289776 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 3s 18ms/step - loss: 71.0603\n",
      "Epoch 2/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 77.2384\n",
      "Epoch 3/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 82.8417\n",
      "Epoch 4/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 88.7300\n",
      "Epoch 5/100\n",
      "54/54 [==============================] - 1s 18ms/step - loss: 94.1995\n",
      "Epoch 6/100\n",
      "54/54 [==============================] - 1s 18ms/step - loss: 99.8182\n",
      "Epoch 7/100\n",
      "54/54 [==============================] - 1s 18ms/step - loss: 105.3093\n",
      "Epoch 8/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 110.9493\n",
      "Epoch 9/100\n",
      "54/54 [==============================] - 1s 18ms/step - loss: 116.2730\n",
      "Epoch 10/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 122.0306\n",
      "Epoch 11/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 127.4094\n",
      "Epoch 12/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 132.9138\n",
      "Epoch 13/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 138.3054\n",
      "Epoch 14/100\n",
      "54/54 [==============================] - 1s 20ms/step - loss: 144.0751\n",
      "Epoch 15/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 149.3749\n",
      "Epoch 16/100\n",
      "54/54 [==============================] - 1s 18ms/step - loss: 154.8712\n",
      "Epoch 17/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 160.5201\n",
      "Epoch 18/100\n",
      "54/54 [==============================] - 1s 18ms/step - loss: 165.7186\n",
      "Epoch 19/100\n",
      "54/54 [==============================] - 1s 18ms/step - loss: 171.3478\n",
      "Epoch 20/100\n",
      "54/54 [==============================] - 1s 18ms/step - loss: 176.9006\n",
      "Epoch 21/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 182.3959\n",
      "Epoch 22/100\n",
      "54/54 [==============================] - 1s 20ms/step - loss: 187.8686\n",
      "Epoch 23/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 193.3681\n",
      "Epoch 24/100\n",
      "54/54 [==============================] - 1s 18ms/step - loss: 198.8197\n",
      "Epoch 25/100\n",
      "54/54 [==============================] - 1s 18ms/step - loss: 204.3464\n",
      "Epoch 26/100\n",
      "54/54 [==============================] - 1s 18ms/step - loss: 209.9725\n",
      "Epoch 27/100\n",
      "54/54 [==============================] - 1s 18ms/step - loss: 215.3625\n",
      "Epoch 28/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 220.8588\n",
      "Epoch 29/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 226.3121\n",
      "Epoch 30/100\n",
      "54/54 [==============================] - 1s 18ms/step - loss: 231.9298\n",
      "Epoch 31/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 237.1824\n",
      "Epoch 32/100\n",
      "54/54 [==============================] - 1s 27ms/step - loss: 242.7042\n",
      "Epoch 33/100\n",
      "54/54 [==============================] - 1s 20ms/step - loss: 248.2554\n",
      "Epoch 34/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 253.7138\n",
      "Epoch 35/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 259.2768\n",
      "Epoch 36/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 264.6472\n",
      "Epoch 37/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 270.2482\n",
      "Epoch 38/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 275.6877\n",
      "Epoch 39/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 281.2085\n",
      "Epoch 40/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 286.4044\n",
      "Epoch 41/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 291.8840\n",
      "Epoch 42/100\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 297.6270\n",
      "Epoch 43/100\n",
      "54/54 [==============================] - 1s 23ms/step - loss: 302.9619\n",
      "Epoch 44/100\n",
      "54/54 [==============================] - 1s 20ms/step - loss: 308.5722\n",
      "Epoch 45/100\n",
      "54/54 [==============================] - 1s 26ms/step - loss: 314.1869\n",
      "Epoch 46/100\n",
      "54/54 [==============================] - 1s 26ms/step - loss: 319.2829\n",
      "Epoch 47/100\n",
      "54/54 [==============================] - 1s 20ms/step - loss: 324.9680\n",
      "Epoch 48/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 330.4658\n",
      "Epoch 49/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 335.9609\n",
      "Epoch 50/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 341.2729\n",
      "Epoch 51/100\n",
      "54/54 [==============================] - 1s 20ms/step - loss: 346.8542\n",
      "Epoch 52/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 352.4930\n",
      "Epoch 53/100\n",
      "54/54 [==============================] - 1s 23ms/step - loss: 357.8019\n",
      "Epoch 54/100\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 363.3508\n",
      "Epoch 55/100\n",
      "54/54 [==============================] - 1s 20ms/step - loss: 368.6642\n",
      "Epoch 56/100\n",
      "54/54 [==============================] - 1s 25ms/step - loss: 374.2613\n",
      "Epoch 57/100\n",
      "54/54 [==============================] - 1s 21ms/step - loss: 379.9748\n",
      "Epoch 58/100\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 385.1774\n",
      "Epoch 59/100\n",
      "54/54 [==============================] - 1s 21ms/step - loss: 390.9594\n",
      "Epoch 60/100\n",
      "54/54 [==============================] - 1s 21ms/step - loss: 396.4548\n",
      "Epoch 61/100\n",
      "54/54 [==============================] - 1s 20ms/step - loss: 401.8698\n",
      "Epoch 62/100\n",
      "54/54 [==============================] - 1s 23ms/step - loss: 407.4656\n",
      "Epoch 63/100\n",
      "54/54 [==============================] - 1s 23ms/step - loss: 412.7816\n",
      "Epoch 64/100\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 418.1323\n",
      "Epoch 65/100\n",
      "54/54 [==============================] - 1s 24ms/step - loss: 423.8901\n",
      "Epoch 66/100\n",
      "54/54 [==============================] - 1s 23ms/step - loss: 429.2891\n",
      "Epoch 67/100\n",
      "54/54 [==============================] - 1s 21ms/step - loss: 434.6166\n",
      "Epoch 68/100\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 440.2477\n",
      "Epoch 69/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 445.6229\n",
      "Epoch 70/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 451.3289\n",
      "Epoch 71/100\n",
      "54/54 [==============================] - 1s 20ms/step - loss: 456.5901\n",
      "Epoch 72/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 462.0714\n",
      "Epoch 73/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 467.5613\n",
      "Epoch 74/100\n",
      "54/54 [==============================] - 1s 20ms/step - loss: 473.1076\n",
      "Epoch 75/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 478.5732\n",
      "Epoch 76/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 483.9377\n",
      "Epoch 77/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 489.6971\n",
      "Epoch 78/100\n",
      "54/54 [==============================] - 1s 21ms/step - loss: 495.1345\n",
      "Epoch 79/100\n",
      "54/54 [==============================] - 1s 21ms/step - loss: 500.6525\n",
      "Epoch 80/100\n",
      "54/54 [==============================] - 1s 21ms/step - loss: 506.0617\n",
      "Epoch 81/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 511.3505\n",
      "Epoch 82/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 517.2074\n",
      "Epoch 83/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 522.5574\n",
      "Epoch 84/100\n",
      "54/54 [==============================] - 1s 20ms/step - loss: 528.1753\n",
      "Epoch 85/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 533.5610\n",
      "Epoch 86/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 539.1586\n",
      "Epoch 87/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 544.5935\n",
      "Epoch 88/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 550.3126\n",
      "Epoch 89/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 555.7577\n",
      "Epoch 90/100\n",
      "54/54 [==============================] - 1s 20ms/step - loss: 561.3326\n",
      "Epoch 91/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 567.0095\n",
      "Epoch 92/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 572.2829\n",
      "Epoch 93/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 577.9201\n",
      "Epoch 94/100\n",
      "54/54 [==============================] - 1s 20ms/step - loss: 583.5209\n",
      "Epoch 95/100\n",
      "54/54 [==============================] - 1s 21ms/step - loss: 588.7683\n",
      "Epoch 96/100\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 594.3745\n",
      "Epoch 97/100\n",
      "54/54 [==============================] - 1s 20ms/step - loss: 600.0116\n",
      "Epoch 98/100\n",
      "54/54 [==============================] - 1s 20ms/step - loss: 605.3490\n",
      "Epoch 99/100\n",
      "54/54 [==============================] - 1s 20ms/step - loss: 611.1941\n",
      "Epoch 100/100\n",
      "54/54 [==============================] - 1s 22ms/step - loss: 616.6409\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f22257b31f0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Compila o modelo com função de perda e otimizador\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Treina o modelo com os dados de treinamento\n",
    "model.fit(training_padded, training_labels, epochs=100, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbf1b2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_text(model, tokenizer, max_sequence_len, seed_text, next_words):\n",
    "    # Faz um loop para gerar cada palavra na sequência\n",
    "    for _ in range(next_words):\n",
    "        # Converte a sequência em números\n",
    "        seed_sequence = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        # Preenche a sequência com zeros até o tamanho máximo\n",
    "        seed_padded = pad_sequences([seed_sequence], maxlen=max_sequence_len, padding='post')\n",
    "        # Faz a predição da próxima palavra usando o modelo\n",
    "        predicted = model.predict(seed_padded, verbose=0)\n",
    "        # Encontra o índice da palavra com maior probabilidade\n",
    "        predicted_word_index = np.argmax(predicted)\n",
    "        # Converte o índice da palavra em uma palavra real\n",
    "        predicted_word = tokenizer.index_word[predicted_word_index]\n",
    "        # Adiciona a palavra predita à sequência de entrada\n",
    "        seed_text += \" \" + predicted_word\n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37ef401e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of of of of of of of of of of\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"\"\n",
    "generated_text = generate_text(model, tokenizer, max_sequence_len, seed_text, 10)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45a633d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be that is the of of of of of of of of of of\n"
     ]
    }
   ],
   "source": [
    "# Define a frase inicial\n",
    "seed_text = \"To be or not to be that is the\"\n",
    "\n",
    "# Define o número de palavras a serem geradas\n",
    "next_words = 10\n",
    "\n",
    "# Gera a sequência de palavras preditas pelo modelo\n",
    "generated_text = generate_text(model, tokenizer, max_sequence_len, seed_text, next_words)\n",
    "\n",
    "# Imprime a sequência de palavras geradas\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3d5b37",
   "metadata": {},
   "source": [
    "# Comentários"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89c9720",
   "metadata": {},
   "source": [
    "### Este projeto eu tentei criar uma rede neural recorrente para a geração de textos, porém aparentemente o errei em alguma das funções de pre-processamento, porque ao final do treino da rede nerual, ele apenas escreve a mesma palavra, ainda não consegui resolver o problema."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
